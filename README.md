# ğŸŒ Advanced_Wikipedia-Text-Crawler  
### *A High-Performance Async Web Crawler for Clean Text Extraction*

The **Advanced Wikipedia Text Crawler** is a powerful, asynchronous Python-based web crawler designed to extract **clean, structured, and high-quality textual data** from Wikipedia at scale.

Built with **asyncio and aiohttp**, this crawler goes beyond traditional scraping by implementing **infinite BFS traversal, intelligent filtering, and duplicate detection**, making it ideal for **NLP datasets, AI model training, and research applications**.

---

<p align="center">
  <strong>âš¡ WikiCrawler AI</strong><br/>
  <em>Fast â€¢ Intelligent â€¢ Scalable Data Extraction</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.x-blue?style=flat-square&logo=python"/>
  <img src="https://img.shields.io/badge/Async-aiohttp%20%7C%20asyncio-green?style=flat-square"/>
  <img src="https://img.shields.io/badge/Web%20Scraping-BeautifulSoup-orange?style=flat-square"/>
  <img src="https://img.shields.io/badge/NLP-Ready-lightgrey?style=flat-square"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square"/>
</p>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Workflow](#-workflow)
- [Technology Stack](#-technology-stack)
- [Core Components](#-core-components)
- [Configuration](#-configuration)
- [Project Structure](#-project-structure)
- [How to Run](#-how-to-run)
- [Use Cases](#-use-cases)
- [Future Enhancements](#-future-enhancements)

---

## ğŸŒŸ Overview

This project is designed to build a **scalable and intelligent web crawler** capable of extracting **clean textual data from Wikipedia** for use in:

- ğŸ§  NLP model training  
- ğŸ¤– AI dataset generation  
- ğŸ“š Knowledge base creation  
- ğŸ” Research and analysis  

Unlike basic crawlers, it ensures:

- High-speed asynchronous execution  
- Clean and meaningful text extraction  
- Duplicate-free dataset generation  
- Controlled and domain-restricted crawling  

---

## âœ¨ Key Features

| Feature | Description |
|--------|------------|
| âš¡ **Asynchronous Crawling** | Uses `aiohttp` & `asyncio` for fast, non-blocking requests |
| ğŸ” **Infinite BFS Traversal** | Structured crawling using queue-based exploration |
| ğŸ§¹ **Advanced Text Cleaning** | Removes HTML, noise, and unwanted symbols |
| ğŸ§  **Smart Filtering** | Eliminates low-quality and noisy text |
| ğŸ” **Duplicate Detection** | Uses MD5 hashing to avoid repeated content |
| ğŸŒ **Domain Restriction** | Crawls only relevant Wikipedia pages |
| ğŸ“„ **Text Output** | Saves clean data into structured text file |

---

## ğŸ— Architecture

The crawler follows a **modular async architecture**:

```
URL Queue â†’ Async Fetch â†’ HTML Parsing â†’ Text Cleaning â†’ Filtering â†’ Deduplication â†’ Storage
```

### Components:

1. **Queue System (BFS)** â†’ Manages crawling order  
2. **Async Fetch Layer** â†’ Handles concurrent requests  
3. **Parsing Layer** â†’ Extracts relevant content  
4. **Cleaning Layer** â†’ Removes noise and unwanted data  
5. **Filtering Layer** â†’ Ensures quality  
6. **Storage Layer** â†’ Saves processed data  

---

## ğŸ”„ Workflow

```
1. Start from initial Wikipedia URL
2. Validate and normalize URL
3. Fetch page asynchronously
4. Extract paragraph text
5. Clean and normalize text
6. Filter low-quality content
7. Remove duplicates using hashing
8. Save cleaned text to file
9. Extract internal links
10. Add links to BFS queue
11. Repeat continuously
```

---

## ğŸ›  Technology Stack

| Component | Technology | Purpose |
|----------|-----------|--------|
| **Language** | Python | Core logic |
| **Async Framework** | asyncio, aiohttp | Concurrent requests |
| **HTML Parsing** | BeautifulSoup | Content extraction |
| **Text Processing** | re, unicodedata | Cleaning & normalization |
| **Hashing** | hashlib | Duplicate detection |
| **NLP Ready** | nltk | Tokenization support |

---

## ğŸ“¦ Core Components

### ğŸ“Œ `fetch()`
- Handles async HTTP requests  
- Includes retry mechanism  
- Manages timeouts  

---

### ğŸ“Œ `extract_visible_text()`
- Extracts `<p>` tag content  
- Removes unwanted HTML elements  
- Converts text to readable format  

---

### ğŸ“Œ `extract_links()`
- Extracts valid internal Wikipedia links  
- Filters irrelevant pages  
- Normalizes URLs  

---

### ğŸ“Œ `clean_text()`
- Applies regex-based cleaning  
- Removes noise and unwanted characters  

---

### ğŸ“Œ `is_useful()`
- Filters text based on:
  - Length  
  - Word count  
  - Noise patterns  
  - Numeric ratio  

---

### ğŸ“Œ `crawl_infinite()`
- Core engine of crawler  
- BFS traversal using queue  
- Handles:
  - URL tracking  
  - Deduplication  
  - Continuous crawling  

---

## âš™ï¸ Configuration

```python
START_URL = "https://en.wikipedia.org/wiki/Argo_(oceanography)"
RATE_LIMIT_DELAY = 0.5
OUTPUT_FILE = "wikipedia_text_only.txt"
```

### Customizable Options:
- Starting URL  
- Crawl speed (rate limit)  
- Output file name  

---

## ğŸ“ Project Structure

```
AdvancedWikipediaTextCrawler.py   # Main crawler script
wikipedia_text_only.txt           # Output dataset
```

---

## âš¡ How to Run

### 1ï¸âƒ£ Install Dependencies
```bash
pip install aiohttp beautifulsoup4 nltk nest_asyncio
```

### 2ï¸âƒ£ Run Script
```bash
python AdvancedWikipediaTextCrawler.py
```

### 3ï¸âƒ£ Output File
```
wikipedia_text_only.txt
```

---

## ğŸ¯ Use Cases

- ğŸ§  NLP dataset creation  
- ğŸ¤– AI model training  
- ğŸ“š Knowledge base generation  
- ğŸ” Text mining projects  
- ğŸ“„ Research and analysis  

---

## ğŸŒŸ Highlights

âœ” High-speed asynchronous crawling  
âœ” Infinite BFS traversal strategy  
âœ” Advanced text cleaning pipeline  
âœ” Duplicate-free dataset generation  
âœ” Scalable and production-ready design  

---

## âš ï¸ Important Notes

- Intended for **educational and research purposes**  
- Respect **Wikipedia scraping policies**  
- Use rate limiting responsibly  

---

## ğŸ”® Future Enhancements

- ğŸ“‚ Export structured JSON datasets  
- ğŸ§  Integrate NLP pipelines (spaCy, transformers)  
- âš¡ Hybrid multi-threaded + async system  
- ğŸŒ Build API or GUI interface  
- ğŸ“Š Add crawl depth control  

---

## ğŸ‘¨â€ğŸ’» Author

**Vaibhav Sharma**  
*Python Developer | Data & AI Enthusiast*

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¡ Final Note

> Building intelligent data pipelines is the backbone of modern AI systems.

This project demonstrates how to create a **scalable, efficient, and production-ready web crawler** using modern Python techniques ğŸš€

---

<p align="center">
  Built with â¤ï¸ using Python & Async Programming<br/>
  <strong>WikiCrawler AI</strong> â€” Powering Intelligent Data Collection
</p>
