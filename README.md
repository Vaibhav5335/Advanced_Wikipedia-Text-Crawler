# Advanced_Wikipedia-Text-Crawler
It is made for data crawling for model's training that is essential in which data given to algorithm for training the model is to be accurately correct and clean.

# ğŸŒ Advanced Wikipedia Text Crawler (Async BFS Version)

### High-Performance Python Web Crawler for Clean Text Extraction

---

## ğŸ“Œ Overview

The **Advanced Wikipedia Text Crawler** is a **high-performance, asynchronous web crawler** built in Python that extracts **clean, structured textual data from Wikipedia pages**.

Unlike basic scrapers, this crawler implements:

* âš¡ **Asynchronous requests (aiohttp)**
* ğŸ” **Infinite BFS (Breadth-First Search) crawling**
* ğŸ§¹ **Advanced text cleaning & filtering**
* ğŸ§  **Duplicate detection using hashing**
* ğŸŒ **Domain-restricted intelligent crawling**

This makes it suitable for **large-scale data collection, NLP pipelines, and research applications**.

---

## ğŸš€ Key Features

### âš¡ Asynchronous Crawling

* Uses `aiohttp` + `asyncio` for non-blocking requests
* Handles multiple pages efficiently

---

### ğŸ” BFS-Based Infinite Crawling

* Uses queue (`deque`) for traversal
* Continuously explores new Wikipedia pages
* Ensures structured crawling flow

---

### ğŸ§¹ Advanced Text Cleaning

* Removes:

  * HTML tags
  * URLs
  * Emails
  * Special characters
* Normalizes text to ASCII
* Converts text to lowercase

---

### ğŸ§  Smart Filtering System

* Removes:

  * Very short text
  * Noise / repeated characters
  * Numeric-heavy content

---

### ğŸ” Duplicate Detection

* Uses **MD5 hashing** to avoid duplicate paragraphs

---

### ğŸŒ Domain Restriction

* Crawls only **Wikipedia domain**
* Skips unwanted links like:

  * `/wiki/File:`
  * `/wiki/Category:`
  * `/wiki/Help:`

---

### ğŸ“„ Output Storage

* Saves cleaned text into:

```id="outputfile"
wikipedia_text_only.txt
```

---

## ğŸ—ï¸ Project Structure

```id="crawlerstructure"
AdvancedWikipediaTextCrawler.py   # Main script
wikipedia_text_only.txt           # Output file (generated)
```

---

## ğŸ–¥ï¸ Tech Stack

### ğŸ Core Language

* Python

### âš™ï¸ Libraries Used

* `aiohttp` â†’ Async HTTP requests
* `asyncio` â†’ Event loop management
* `BeautifulSoup` â†’ HTML parsing
* `re` â†’ Regex-based cleaning
* `unicodedata` â†’ Text normalization
* `hashlib` â†’ Duplicate detection
* `nltk` â†’ NLP support (tokenization ready)

---

## âš™ï¸ Configuration

```python id="configblock"
START_URL = "https://en.wikipedia.org/wiki/Argo_(oceanography)"
RATE_LIMIT_DELAY = 0.5
OUTPUT_FILE = "wikipedia_text_only.txt"
```

### ğŸ”§ Customizable:

* Starting page
* Crawl speed (rate limit)
* Output file name

---

## ğŸ”„ Working Pipeline

```id="pipelineflow"
1. Start from initial Wikipedia URL
2. Normalize and validate URL
3. Fetch page asynchronously
4. Extract visible paragraph text
5. Clean and filter text
6. Hash text to remove duplicates
7. Save valid text to file
8. Extract internal Wikipedia links
9. Add new links to BFS queue
10. Repeat infinitely
```

---

## ğŸ“œ Core Components Explained

### ğŸ“Œ `fetch()`

* Async HTTP request handler
* Retry mechanism (3 attempts)
* Handles timeouts and failures

---

### ğŸ“Œ `extract_visible_text()`

* Removes unwanted HTML elements
* Extracts `<p>` tag content
* Converts text to clean ASCII format

---

### ğŸ“Œ `extract_links()`

* Extracts internal Wikipedia links
* Filters irrelevant pages
* Normalizes URLs

---

### ğŸ“Œ `clean_text()`

* Removes noise using regex
* Ensures readable output

---

### ğŸ“Œ `is_useful()`

Filters text based on:

* Length
* Word count
* Numeric ratio
* Noise patterns

---

### ğŸ“Œ `crawl_infinite()`

* Core engine of crawler
* BFS traversal using queue
* Handles:

  * URL tracking
  * Text deduplication
  * Continuous crawling

---

## âš¡ How to Run

### 1ï¸âƒ£ Install Dependencies

```bash id="installcrawler"
pip install aiohttp beautifulsoup4 nltk nest_asyncio
```

---

### 2ï¸âƒ£ Run Script

```bash id="runcrawler"
python AdvancedWikipediaTextCrawler.py
```

---

### 3ï¸âƒ£ Output

* Extracted text will be saved in:

```
wikipedia_text_only.txt
```

---

## ğŸ“Š Use Cases

* ğŸ§  NLP dataset creation
* ğŸ“š Knowledge base generation
* ğŸ¤– Chatbot training data
* ğŸ“„ Text mining projects
* ğŸ” Research and analysis

---

## ğŸŒŸ Highlights

âœ” Asynchronous high-speed crawler
âœ” Infinite BFS traversal
âœ” Advanced text cleaning pipeline
âœ” Duplicate-free dataset generation
âœ” Scalable & production-ready logic

---

## âš ï¸ Important Notes

* Designed for **educational and research use**
* Always respect **Wikipediaâ€™s usage policies**
* Use rate limiting responsibly

---

## ğŸ§© Future Enhancements

* ğŸ“‚ Save structured JSON output
* ğŸ§  Integrate NLP pipelines (spaCy, transformers)
* âš¡ Multi-threaded + async hybrid
* ğŸŒ Add GUI or API interface
* ğŸ“Š Add crawl depth control

---

## ğŸ‘¨â€ğŸ’» Author

**Vaibhav Sharma**

* Python Developer | Data & AI Enthusiast
* Specialized in Web Scraping & Automation

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¡ Final Note

This project goes beyond basic scraping and demonstrates how to build a **scalable, intelligent crawler system** using modern Python techniques.

A powerful addition to any **Data Science / Backend / AI portfolio ğŸš€**

---
